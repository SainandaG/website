import asyncio
from typing import Dict, Any, List
from app.services.db_connector import db_connector
from app.services.schema_analyzer import schema_analyzer

class MetricsService:
    def __init__(self):
        pass

    async def get_global_metrics(self, connection_id: str) -> Dict[str, Any]:
        """
        Calculate real-time global metrics identifying 'Fact' tables 
        and aggregating their statistics.
        """
        print(f"üìä MetricsService: Calculating global stats for {connection_id}...")
        
        # Default fallback
        metrics = {
            'transaction_rate': 0,
            'total_transactions': 0,
            'average_amount': 0,
            'fraud_alerts': 0,
            'failed_transactions': 0
        }

        try:
            schema = schema_analyzer.get_analysis_result(connection_id)
            if not schema:
                return metrics

            # 1. Identify Fact/Transaction Tables
            # We look for tables tagged as 'fact' or with high row counts
            fact_tables = []
            tables = schema.tables if hasattr(schema, 'tables') else schema.get('tables', [])
            
            for t in tables:
                t_name = getattr(t, 'name', t.get('name'))
                t_type = getattr(t, 'table_type', t.get('table_type', 'unknown'))
                row_count = getattr(t, 'row_count', t.get('row_count', 0))
                
                # Heuristic: It's a fact table if AI said so OR if it has massive rows
                if t_type == 'fact' or row_count > 10000:
                    fact_tables.append(t)

            if not fact_tables:
                # Fallback: Just take the largest table
                print("‚ö†Ô∏è No explicit fact tables found. Using largest table as proxy.")
                sorted_tables = sorted(tables, key=lambda x: getattr(x, 'row_count', x.get('row_count', 0)), reverse=True)
                if sorted_tables:
                    fact_tables.append(sorted_tables[0])

            # 2. Aggregate Metrics from Fact Tables
            total_tx = 0
            total_amount_sum = 0
            total_amount_count = 0
            
            for t in fact_tables:
                t_name = getattr(t, 'name', t.get('name'))
                t_rows = getattr(t, 'row_count', t.get('row_count', 0))
                total_tx += t_rows

                # Find an 'amount' column for currency metrics
                numeric_cols = getattr(t, 'numeric_columns', t.get('numeric_columns', []))
                amount_col = next((c for c in numeric_cols if any(x in c.lower() for x in ['amount', 'price', 'value', 'total'])), None)
                
                if amount_col:
                    try:
                        # Fetch AVG of this column
                        # We use a safe sample query or actual avg depending on DB size
                        # For speed, let's just query AVG directly (DBs are usually optimized for this)
                        sql = f"SELECT AVG({amount_col}) as avg_val FROM {t_name}"
                        # Check DB type to avoid syntax errors? Basic SQL usually works.
                        # We'll wrap in try/except per table
                        result = await db_connector.query(connection_id, sql)
                        if result and result[0].get('avg_val'):
                            avg_val = float(result[0]['avg_val'])
                            total_amount_sum += (avg_val * t_rows) # Weighted sum
                            total_amount_count += t_rows
                    except Exception as e:
                        print(f"‚ö†Ô∏è Failed to calc average for {t_name}.{amount_col}: {e}")

            # 3. Calculate Final Metrics
            metrics['total_transactions'] = total_tx
            
            if total_amount_count > 0:
                metrics['average_amount'] = round(total_amount_sum / total_amount_count, 2)
            
            # Simulate TPS based on total volume (assuming uniform distribution over 30 days for demo feel, or just heuristic)
            # Real TPS would require timestamp analysis which is expensive for a quick dash.
            metrics['transaction_rate'] = min(round(total_tx / 86400, 2), 9999) if total_tx > 0 else 0
            if metrics['transaction_rate'] == 0 and total_tx > 0: metrics['transaction_rate'] = 1.5 # Min activity

            # Mock Alerts based on "Risk" tables if any
            alerts = 0
            for t in tables:
                t_name = getattr(t, 'name', t.get('name')).lower()
                if 'fraud' in t_name or 'alert' in t_name or 'risk' in t_name:
                    alerts += getattr(t, 'row_count', t.get('row_count', 0))
            metrics['fraud_alerts'] = alerts

            print(f"‚úÖ Calculated Global Metrics: {metrics}")
            return metrics

        except Exception as e:
            print(f"‚ùå Metrics Calculation Error: {e}")
            return metrics

metrics_service = MetricsService()
